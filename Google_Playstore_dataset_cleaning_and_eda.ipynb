{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install scikit-posthocs"
      ],
      "metadata": {
        "id": "Pa0TuoPhhmqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, we'll import the necessary Python libraries for data manipulation, statistical analysis, and visualization. We are also installing `scikit-posthocs` for advanced statistical testing later in the analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "K3qVWx28xnei"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPrNMcrKxLDl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import kruskal\n",
        "import scikit_posthocs as sp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Executive Summary\n",
        "\n",
        "This report presents a comprehensive analysis of the Google Play Store dataset to uncover key factors driving app success, focusing on user ratings, number of installs, and app category.\n",
        "\n",
        "Methodology: The project involved a rigorous data cleaning and imputation process. A critical challenge was a corrupted data row, which was identified and removed. Missing Rating and Size values were not missing at random; a Kruskal-Wallis test confirmed a statistically significant difference in their distributions across app categories (p < 0.05). Consequently, a context-aware imputation strategy using category-specific medians was employed.\n",
        "\n",
        "Key Findings:\n",
        "\n",
        "1. App category seems to play a significant role on performance. 'GAME' and 'COMMUNICATION' apps dominate in total installations, while 'HEALTH_AND_FITNESS', 'BOOKS_AND_REFERENCE' and 'EVENTS' apps achieve statistically higher user ratings.\n",
        "\n",
        "2. The 'Dating' Anomaly: The 'DATING' category consistently exhibits statistically lower user ratings than nearly all other categories, suggesting a unique market dynamic or user expectation gap.\n",
        "\n",
        "3. Installs Correlate with Reviews: A strong positive correlation exists between the number of installs and the number of reviews, confirming that user engagement through reviews grows with an app's user base."
      ],
      "metadata": {
        "id": "PE3eTFw9PZR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Loading and Initial Inspection\n",
        "\n",
        "The first step in any data analysis project is to load the data and perform a high-level inspection. This helps us understand the dataset's structure, the data types of each column, and get a preliminary look at the data itself."
      ],
      "metadata": {
        "id": "g4vora_nPpKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset from the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv('googleplaystore.csv')"
      ],
      "metadata": {
        "id": "39FWWeRfxP94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 5 rows to get a feel for the data and its columns\n",
        "df.head()"
      ],
      "metadata": {
        "id": "2QRA-xtIxjyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a concise summary of the DataFrame.\n",
        "# This includes the number of non-null values and the data type of each column.\n",
        "df.info()"
      ],
      "metadata": {
        "id": "r5S1ImIDxxCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial Observation: The .info() output shows that several columns, such as Rating, Current Ver and Android Ver, have fewer than 10841 non-null entries, indicating the presence of missing values. Additionally, columns that should be numeric like Reviews, Size, Installs, and Price are currently of the object data type. This tells us that data cleaning and type conversion will be necessary."
      ],
      "metadata": {
        "id": "AfEiTwYRQMNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate descriptive statistics for the numerical columns.\n",
        "# Note: At this stage, only 'Rating' is numeric, so the output is limited.\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "d8kln2GwyN1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a count of missing values in each column to quantify the cleaning task ahead.\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "0URV3wJazaOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Cleaning (First Pass)\n",
        "\n",
        "Based on our initial inspection, the data requires significant cleaning. In this first pass, we will address the following issues based on our initial understanding of the data:\n",
        "- Duplicated rows\n",
        "- Incorrect data types (e.g., 'Reviews', 'Size')\n",
        "- Special characters and non-numeric values in numeric columns\n"
      ],
      "metadata": {
        "id": "vnGJiV7S3QMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Handling Duplicated Rows\n",
        "\n",
        "Duplicate entries can skew analysis and should be removed. We'll first count them and then drop them from the DataFrame."
      ],
      "metadata": {
        "id": "xRXKpEPidvP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the total number of duplicated rows in the dataset\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "NUSH2Q61dylJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the duplicated rows, keeping the first occurrence\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "u7fvuUM6d3L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Cleaning and Converting Columns\n",
        "\n",
        "Next, we will iterate through the columns identified as having incorrect data types or special characters, converting them to a usable format.\n",
        "\n",
        "#### Reviews Column"
      ],
      "metadata": {
        "id": "NE837usk3cT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'Reviews' column should be numeric, but is an 'object' type.\n",
        "# Let's check for any non-numeric values that might be causing this.\n",
        "df[~df['Reviews'].str.isnumeric()]"
      ],
      "metadata": {
        "id": "BUmg8qio0Kkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: The check reveals a non-numeric value '3.0M' in the 'Reviews' column for the app at index 10472. This seems to be a data entry error where the 'M' for million was included. We will correct this specific entry."
      ],
      "metadata": {
        "id": "kZfGnMTrR1TZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The non-numeric value appears to be '3.0M'. We will manually correct this to '3000000'.\n",
        "# Note: This is our first encounter with the problematic row 10472.\n",
        "df.loc[10472, 'Reviews'] = '3000000'"
      ],
      "metadata": {
        "id": "EoU49u-x3lCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that all values are numeric strings, we can safely convert the column to an integer type.\n",
        "df['Reviews'] = df['Reviews'].astype(int)"
      ],
      "metadata": {
        "id": "sua7QBMC5ax1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Size Column\n",
        "The `Size` column contains a mix of values in kilobytes ('k'), megabytes ('M'), and the string 'Varies with device'. We will standardize these to kilobytes (KB) and handle the special cases.\n"
      ],
      "metadata": {
        "id": "weUfXzVi7HSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check how the data in this 'Size' column look like\n",
        "df['Size'].unique()"
      ],
      "metadata": {
        "id": "O0CVp3ja7E44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# At first sight we see that almost all sizes are expressed as Megabytes (M)\n",
        "# and Kilobytes (k). There are two major issues. Values that are expressed as\n",
        "# 'Varies with device' and '1,000+'.\n",
        "\n",
        "# First we'll try to convert Megabytes to Kilobytes and handling the issues\n",
        "# by applying a function.\n",
        "\n",
        "\n",
        "# This function will parse the 'Size' string and convert it to a numeric value in KB.\n",
        "# - It replaces 'Varies with device' with NaN (Not a Number) as the size is unknown.\n",
        "# - It handles a specific outlier '1,000+' which appears to be a data entry error.\n",
        "# - It converts 'M' (megabytes) to kilobytes by multiplying by 1000.\n",
        "# - It removes 'k' from kilobyte values and converts to float.\n",
        "def convert_to_kb(value):\n",
        "  \"\"\"\n",
        "  Convert size string to kilobytes\n",
        "  Handles cases like: '19M', '201k', 'Varies with device', '1,000+'\n",
        "  \"\"\"\n",
        "\n",
        "  cases_with_issue = ['Varies with device']\n",
        "  if value in cases_with_issue:\n",
        "    return np.nan\n",
        "\n",
        "  # In cases where value == '1,000+', replace it with 1 (1 Kilobyte)\n",
        "  if value == '1,000+':\n",
        "    return 1\n",
        "\n",
        "  # Handle other cases\n",
        "  if 'M' in value:\n",
        "    number = float(value.replace('M', ''))\n",
        "    return number * 1000\n",
        "  elif 'k' in value:\n",
        "    return float(value.replace('k', ''))\n",
        "  else:\n",
        "    return float(value)"
      ],
      "metadata": {
        "id": "3BUwLoqcJSNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the 'convert_to_kb' function to 'Size' column\n",
        "df['Size'] = df['Size'].apply(convert_to_kb).round(2)"
      ],
      "metadata": {
        "id": "uXh3KPhP7Xg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installs and Price Columns\n",
        "The `Installs` and `Price` columns contain special characters (like '+', ',', '$') that prevent them from being treated as numeric. We will remove these characters and convert the columns to the appropriate numeric types."
      ],
      "metadata": {
        "id": "ZU8Si2tMFdfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for all the unique values in 'Installs' column\n",
        "df['Installs'].unique()"
      ],
      "metadata": {
        "id": "E2OVksgpCB2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace '+', ',' and 'Free' with '' and 0\n",
        "df['Installs'] = df['Installs'].str.replace('+', '')\n",
        "df['Installs'] = df['Installs'].str.replace(',', '')\n",
        "df['Installs'] = df['Installs'].str.replace('Free', '0')"
      ],
      "metadata": {
        "id": "WzN6G8AyDKQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Installs' column into integers\n",
        "df['Installs'] = df['Installs'].astype(int)"
      ],
      "metadata": {
        "id": "uEETxzUDHpZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for all the unique values in 'Price' column\n",
        "df['Price'].unique()"
      ],
      "metadata": {
        "id": "8OnDAsj3HWwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace '$' and 'Everyone' with '' and '0' respectively\n",
        "df['Price'] = df['Price'].str.replace('$', '')\n",
        "df['Price'] = df['Price'].str.replace('Everyone', '0')"
      ],
      "metadata": {
        "id": "fOzQK3tiHZF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Price' column into floats\n",
        "df['Price'] = df['Price'].astype(float)"
      ],
      "metadata": {
        "id": "6k7goqJKH8hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Last Updated Column\n",
        "This column will be converted to a proper datetime format, from which we can extract the day, month, and year for potential feature engineering."
      ],
      "metadata": {
        "id": "sqzR1z-5QrlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We first check for the unique values in 'Last Updated' column\n",
        "df['Last Updated'].unique()"
      ],
      "metadata": {
        "id": "6FRFrMGWQud_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check also for nan values\n",
        "df['Last Updated'].isnull().sum()"
      ],
      "metadata": {
        "id": "Jv8b_T8oQubw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'Last Updated' column to datetime objects.\n",
        "# The `errors='coerce'` argument is crucial; it will turn any unparseable date strings into NaT (Not a Time).\n",
        "df['Last Updated'] = pd.to_datetime(df['Last Updated'], format='mixed', errors='coerce')"
      ],
      "metadata": {
        "id": "9qSdfvC-Wg6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After that we create three new columns: 'Last_Updated_Day', 'Last_Updated_Month'\n",
        "# and 'Last_Updated_Year'.\n",
        "df['Last_Updated_Day'] = df['Last Updated'].dt.day.astype('Int64')\n",
        "df['Last_Updated_Month'] = df['Last Updated'].dt.month.astype('Int64')\n",
        "df['Last_Updated_Year'] = df['Last Updated'].dt.year.astype('Int64')"
      ],
      "metadata": {
        "id": "EPPj9zzqUEFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# And drop the 'Last Updated' column\n",
        "df.drop('Last Updated', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "6HIK5mpPUEAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Type"
      ],
      "metadata": {
        "id": "_84bghp9XyhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the unique values in 'Type' feature\n",
        "df['Type'].unique()"
      ],
      "metadata": {
        "id": "Dh0TayahX9Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As we can see there is also the '0' value which probably refers to 'free'\n",
        "# Let's replace it with 'Free'\n",
        "df['Type'] = df['Type'].str.replace('0', 'Free')"
      ],
      "metadata": {
        "id": "WN341nb_YEYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Investigating an Anomaly in the 'Category' Column\n",
        "\n",
        "During our systematic cleaning, we perform a final check on the `Category` column, which should contain a fixed set of text values."
      ],
      "metadata": {
        "id": "Rl-plFhhZN6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's examine all unique values in the 'Category' column to ensure they are valid.\n",
        "df['Category'].unique()"
      ],
      "metadata": {
        "id": "hFI8tkB4ZSU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique values reveal a highly unusual entry: '1.9'. This should not be a category.\n",
        "# Let's isolate the row(s) containing this value to understand the context.\n",
        "df[df['Category'] == '1.9']"
      ],
      "metadata": {
        "id": "2Uy_RpO5ZSSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. The Root Cause: A Corrupted Row\n",
        "\n",
        "**Major Discovery:** The investigation into the anomalous '1.9' category leads us to row 10472. A closer look at this row reveals that it is the source of many of the data inconsistencies we've been manually fixing.\n",
        "\n",
        "The data in this row appears to have been shifted one column to the left, starting from the `Category` column which is missing. This caused '1.9' (the `Rating`) to be read as the `Category`, '3.0M' (the `Reviews`) to be read as the `Rating`, and so on.\n",
        "\n",
        "This explains the strange values we encountered:\n",
        "- The non-numeric '3.0M' in the `Reviews` column.\n",
        "- The 'Everyone' value in the `Price` column.\n",
        "- The '1,000+' value in the `Size` column.\n",
        "\n",
        "**Decision:** Instead of attempting a complex and potentially error-prone repair of this single row, the most robust and reliable solution is to discard it entirely. Given our large dataset, the removal of one row will have no statistical impact. We will now restart the cleaning process from a fresh load of the data, this time beginning by removing the corrupted row."
      ],
      "metadata": {
        "id": "ix3VNTz-bWOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Final Data Cleaning and Preprocessing\n",
        "\n",
        "Armed with the knowledge that row 10472 is the source of the data corruption, we will now perform a definitive cleaning pass on a freshly loaded dataset. The process is as follows:\n",
        "\n",
        "1.  **Reload** the original dataset.\n",
        "2.  **Remove** the corrupted row (10472) immediately.\n",
        "3.  **Handle duplicates** based on App name, as this is the unique identifier for each application.\n",
        "4.  **Clean and convert** data types for all relevant columns.\n",
        "5.  **Investigate and impute** missing values in the `Rating` and `Size` columns using a statistically-backed strategy.\n",
        "6.  **Handle the few remaining missing values** in other columns."
      ],
      "metadata": {
        "id": "ldtVfK37dGGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the original 'googleplaystore.csv' file to start with a clean slate.\n",
        "df = pd.read_csv('googleplaystore.csv')"
      ],
      "metadata": {
        "id": "KGxrT7vtbUi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Removing Corrupted Data and Duplicates"
      ],
      "metadata": {
        "id": "ys7nAAIZvi6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Immediately drop the identified corrupted row at index 10472.\n",
        "df.drop(10472, inplace=True)\n",
        "\n",
        "# Reset the index to ensure it is contiguous after dropping the row.\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "ZhJLu0KIdcnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate entries based on the 'App' name.\n",
        "# We choose to keep the 'first' occurrence, which typically represents the most recent entry.\n",
        "df.drop_duplicates(subset='App', keep='first', inplace=True)"
      ],
      "metadata": {
        "id": "UYr57CJnelS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Column Cleaning and Type Conversion\n",
        "\n",
        "Now that the primary data integrity issues are resolved, we can proceed with cleaning and converting the columns to their correct data types much more efficiently."
      ],
      "metadata": {
        "id": "Fnf9t4R-v0tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that the 'Reviews' column is now free of non-numeric values.\n",
        "# The output is an empty DataFrame, confirming the column is clean.\n",
        "df[~df['Reviews'].str.isnumeric()]"
      ],
      "metadata": {
        "id": "2vq0IafDelQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'Reviews' column from object to integer.\n",
        "df['Reviews'] = df['Reviews'].astype(int)"
      ],
      "metadata": {
        "id": "DwE7GJKifZru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simplified function to convert 'Size' to kilobytes (KB).\n",
        "# This version no longer needs to handle the specific errors caused by row 10472.\n",
        "def convert_to_kb(value):\n",
        "  \"\"\"\n",
        "  Convert size string to kilobytes\n",
        "  Handles cases like: '19M', '201k', 'Varies with device'\n",
        "  \"\"\"\n",
        "\n",
        "  cases_with_issue = ['Varies with device']\n",
        "  if value in cases_with_issue:\n",
        "    return np.nan\n",
        "\n",
        "  # Handle other cases\n",
        "  if 'M' in value:\n",
        "    number = float(value.replace('M', ''))\n",
        "    return number * 1000\n",
        "  elif 'k' in value:\n",
        "    return float(value.replace('k', ''))\n",
        "  else:\n",
        "    return float(value)\n",
        "\n",
        "# Apply the function and round to two decimal places.\n",
        "df['Size'] = df['Size'].apply(convert_to_kb).round(2)"
      ],
      "metadata": {
        "id": "QzKfjgwjelOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# By checking unique values in 'Installs' column we see that there is no longer\n",
        "# the value 'Free'\n",
        "df['Installs'].unique()"
      ],
      "metadata": {
        "id": "Zdq1U7UGgOy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the 'Installs' column by removing '+' and ',' and convert to integer.\n",
        "df['Installs'] = df['Installs'].str.replace('+', '')\n",
        "df['Installs'] = df['Installs'].str.replace(',', '')\n",
        "df['Installs'] = df['Installs'].astype(int)"
      ],
      "metadata": {
        "id": "ZoubCsI4gOwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the 'Price' column by removing '$' and convert to float.\n",
        "df['Price'] = df['Price'].str.replace('$', '')\n",
        "df['Price'] = df['Price'].astype(float)"
      ],
      "metadata": {
        "id": "31Bw5czQgOuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'Last Updated' to datetime objects for easier analysis.\n",
        "df['Last Updated'] = pd.to_datetime(df['Last Updated'])"
      ],
      "metadata": {
        "id": "dRvPxKsugOsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering: Extract day, month, and year from 'Last Updated'.\n",
        "# This allows for time-based analysis.\n",
        "df['Last_Updated_Day'] = df['Last Updated'].dt.day.astype(int)\n",
        "df['Last_Updated_Month'] = df['Last Updated'].dt.month.astype(int)\n",
        "df['Last_Updated_Year'] = df['Last Updated'].dt.year.astype(int)\n",
        "\n",
        "# Drop the original 'Last Updated' column as it's no longer needed.\n",
        "df.drop('Last Updated', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "VZZVblMOdcfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Missing Value Analysis and Imputation\n",
        "\n",
        "After the initial cleaning, we are left with missing values primarily in the `Rating` and `Size` columns. Simply dropping these rows or filling them with a global mean/median could introduce bias. Therefore, we will first investigate *why* the data is missing and then apply a context-aware imputation strategy."
      ],
      "metadata": {
        "id": "fARrJEQF8IaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the count of missing values now that the main cleaning is complete.\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "CHPFqx1a8RCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Investigating Missing Ratings\n",
        "\n",
        "**Hypothesis:** Ratings are likely missing for new or unpopular apps that have not yet accumulated enough reviews or installs.\n",
        "\n",
        "To test this, we will compare the descriptive statistics of apps *with* ratings versus those *without*."
      ],
      "metadata": {
        "id": "fAKQ1nRl-hdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame of apps where 'Rating' is null.\n",
        "missing_rating_df = df[df['Rating'].isnull()]\n",
        "\n",
        "# Display summary statistics for this group.\n",
        "missing_rating_df[['Reviews', 'Installs', 'Price']].describe()"
      ],
      "metadata": {
        "id": "vkAaa0P1IB_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For comparison, create a DataFrame of apps that have a 'Rating'.\n",
        "has_rating_df = df[df['Rating'].notnull()]\n",
        "\n",
        "# Display summary statistics for this group.\n",
        "has_rating_df[['Reviews', 'Installs', 'Price']].describe()"
      ],
      "metadata": {
        "id": "5j7rxZ9wCleq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** The descriptive statistics clearly support our hypothesis. The group of apps without ratings has a dramatically lower mean and median for both Reviews and Installs compared to the group with ratings. This confirms that the data is not missing at random, and a simple removal or global imputation would be inappropriate."
      ],
      "metadata": {
        "id": "xFLUK-uqxCF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Justifying Imputation Strategy with Statistical Testing\n",
        "\n",
        "Our next hypothesis is that the average rating differs significantly across app categories. If true, this would justify imputing missing ratings with the median rating of their specific category, which is a more accurate approach than using a single global value.\n",
        "\n",
        "First, we visualize the distributions with a boxplot, sorted by median rating.\n"
      ],
      "metadata": {
        "id": "4-zZ2UtgxZNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the median rating for each category to create a sorted order for the plot.\n",
        "category_order = df.dropna(subset=['Rating']).groupby('Category')['Rating'].median().sort_values(ascending=False).index\n",
        "\n",
        "# Create the boxplot to visually inspect the distributions.\n",
        "plt.figure(figsize=(14, 12))\n",
        "\n",
        "sns.boxplot(\n",
        "    data=df,\n",
        "    x='Rating',\n",
        "    y='Category',\n",
        "    order=category_order,\n",
        "    color='royalblue'\n",
        ")\n",
        "\n",
        "# Add some polish and formatting.\n",
        "plt.suptitle(\n",
        "    'Distribution of App Ratings Across Categories',\n",
        "    fontsize=16,\n",
        "    fontweight='bold',\n",
        "    y=1.02,\n",
        "    alpha=.7\n",
        ")\n",
        "plt.xlabel('Rating', fontsize=12)\n",
        "plt.ylabel('Category', fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "sns.despine()\n",
        "\n",
        "ax = plt.gca()\n",
        "tick_locations = ax.get_yticks()\n",
        "\n",
        "formatted_labels = [label.replace('_', ' ').title() for label in category_order]\n",
        "\n",
        "plt.yticks(ticks=tick_locations, labels=formatted_labels)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m9_PxUS6H6hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The boxplots provide a clear summary of the median and spread of ratings for each category. To get a more detailed view of the shape of these distributions, we can visualize them using Kernel Density Estimate (KDE) plots. This will help us see where ratings are most concentrated and if there are any unusual patterns before we proceed with statistical testing."
      ],
      "metadata": {
        "id": "deArQRPM18IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For visual clarity, we'll focus on the top 15 categories by the number of apps.\n",
        "top_categories = df['Category'].value_counts().nlargest(15).index\n",
        "\n",
        "# Filter the DataFrame to only include these top categories.\n",
        "df_top_cat = df[df['Category'].isin(top_categories)]\n",
        "\n",
        "# Create a FacetGrid, which is ideal for creating a grid of similar plots.\n",
        "# We'll arrange the plots in a grid with 5 columns.\n",
        "g = sns.FacetGrid(df_top_cat, col=\"Category\", col_wrap=5, height=2.5, aspect=1.2)\n",
        "\n",
        "# Map a filled Kernel Density Estimate (KDE) plot onto each subplot of the grid.\n",
        "# This provides a smooth representation of the rating distributions.\n",
        "g.map(sns.kdeplot, \"Rating\", fill=True, alpha=0.5, color='royalblue')\n",
        "\n",
        "# Add a descriptive super-title to the entire figure.\n",
        "g.fig.suptitle('Density Distribution of Ratings for Top 15 Categories', y=1.03, fontsize=16, fontweight='bold')\n",
        "\n",
        "# Set individual subplot titles to be the category name.\n",
        "g.set_titles(\"{col_name}\", size=12)\n",
        "\n",
        "# Set the axis labels for clarity.\n",
        "g.set_axis_labels(\"Rating\", \"Density\")\n",
        "\n",
        "# Ensure the layout is clean and titles/labels do not overlap.\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1aUTu_0yywIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visual Observation:** The boxplot suggests that there are indeed differences in the median ratings across categories. However, to confirm this with statistical rigor, we will perform a Kruskal-Wallis H-test. This non-parametric test is appropriate because we cannot assume the data is normally distributed, as KDE plots suggest.\n",
        "\n",
        "- Null Hypothesis (H₀): The median ratings are the same across all app categories.\n",
        "\n",
        "- Alternative Hypothesis (H₁): At least one app category has a different median rating.\n",
        "\n",
        "- Significance Level (α): 0.05"
      ],
      "metadata": {
        "id": "y-7Fs7XXeWk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data for the Kruskal-Wallis test by creating a list of rating series for each category.\n",
        "categories = df['Category'].unique()\n",
        "rating_by_category = [df['Rating'][df['Category'] == cat].dropna() for cat in categories]"
      ],
      "metadata": {
        "id": "z8_Pj-JFeU0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the Kruskal-Wallis H-Test.\n",
        "h_statistic, p_value = kruskal(*rating_by_category)"
      ],
      "metadata": {
        "id": "JtvNXzViiDgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the test results.\n",
        "print(\"--- Kruskal-Wallis H-Test Results ---\")\n",
        "print(f\"H-statistic: {h_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")"
      ],
      "metadata": {
        "id": "gFfn3RG_iDd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since the p-value is significant, we proceed with Dunn's post-hoc test\n",
        "# to identify which specific category pairs are different.\n",
        "# The heatmap visualizes these pairwise comparisons.\n",
        "alpha = 0.05\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"\\nResult: The p-value is less than our significance level of 0.05.\")\n",
        "    print(\"Conclusion: We reject the null hypothesis. There is a statistically significant difference in median ratings across app categories.\")\n",
        "    print(\"\\nProceeding with Dunn's Post-Hoc Test to identify specific differences...\")\n",
        "\n",
        "    dunn_results = sp.posthoc_dunn(\n",
        "        df, val_col='Rating',\n",
        "        group_col='Category',\n",
        "        p_adjust='bonferroni'\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    sns.heatmap(\n",
        "        dunn_results,\n",
        "        annot=False,\n",
        "        cmap='viridis_r',\n",
        "        cbar_kws={'label': 'Adjusted p-value'}\n",
        "    )\n",
        "    plt.suptitle(\n",
        "        \"Pairwise Comparison of App Ratings Across Categories (Dunn's Test)\",\n",
        "        fontsize=16,\n",
        "        y=1.02,\n",
        "        alpha=.7\n",
        "    )\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nResult: The p-value is greater than our significance level of 0.05.\")\n",
        "    print(\"Conclusion: We fail to reject the null hypothesis. There is no statistically significant evidence of a difference in median ratings across categories.\")"
      ],
      "metadata": {
        "id": "5-J3yjXXjUzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:** The Kruskal-Wallis test yields a p-value of 0.0000, which is far below our significance level of 0.05. We therefore **reject the null hypothesis** and conclude that there is a statistically significant difference in median ratings across categories.\n",
        "\n",
        "This result validates our strategy of imputing the missing `Rating` values with the **median rating of each app's respective category.**"
      ],
      "metadata": {
        "id": "2RmD-NSWl1A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the median rating for each category.\n",
        "category_medians = df.groupby('Category')['Rating'].transform('median')\n",
        "\n",
        "# Fill the NaN values in the 'Rating' column using the calculated category medians.\n",
        "df.fillna({'Rating': category_medians}, inplace=True)"
      ],
      "metadata": {
        "id": "RxsuHadnH6e9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.3 Imputing Missing 'Size' Values\n",
        "\n",
        "We will apply the same robust methodology to the missing `Size` values. We first investigate if the missingness is random and then test if app size differs by category to justify a grouped median imputation."
      ],
      "metadata": {
        "id": "U6gUIsqpRB2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame of apps where 'Size' is null.\n",
        "missing_size_df = df[df['Size'].isnull()]\n",
        "\n",
        "# Display summary statistics for this group.\n",
        "missing_size_df[['Reviews', 'Installs', 'Price']].describe()"
      ],
      "metadata": {
        "id": "oqqAgG_VH6c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For comparison, create a DataFrame of apps that have a 'Size'.\n",
        "has_size_df = df[df['Size'].notnull()]\n",
        "\n",
        "# Display summary statistics for this group.\n",
        "has_size_df[['Reviews', 'Installs', 'Price']].describe()"
      ],
      "metadata": {
        "id": "Bcws94WLH6aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation and Hypothesis for Missing 'Size':**\n",
        "\n",
        "The analysis of apps with missing `Size` values (labeled as \"Varies with device\") reveals a pattern that is the inverse of what was observed for missing `Rating` values.\n",
        "\n",
        "-   The **mean and median number of `Installs` and `Reviews` are significantly higher** for apps where the size \"Varies with device\".\n",
        "-   This strongly suggests that this label is not used for small or unpopular apps. Instead, it is likely associated with **highly successful, complex applications**. These apps often use modern distribution methods like Android App Bundles, which deliver optimized packages tailored to each user's device, making a single, fixed size irrelevant.\n",
        "\n",
        "Given this, the missingness in the `Size` column is also **not random**. It is systematically related to the app's complexity and popularity. Therefore, a context-aware imputation strategy is still the most appropriate choice. We will proceed by visualizing the boxplots & distributions and performing a Kruskal-Wallis test to confirm that app size differs significantly across categories, which will justify our plan to impute with the category-specific median."
      ],
      "metadata": {
        "id": "Gy9mp6IA4e_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the median rating for each category to create a sorted order for the plot.\n",
        "size_order = df.dropna(subset=['Size']).groupby('Category')['Size'].median().sort_values(ascending=False).index\n",
        "\n",
        "# Create the boxplot to visually inspect the distributions.\n",
        "plt.figure(figsize=(14, 12))\n",
        "\n",
        "sns.boxplot(\n",
        "    data=df,\n",
        "    x='Size',\n",
        "    y='Category',\n",
        "    order=size_order,\n",
        "    color='royalblue'\n",
        ")\n",
        "\n",
        "# Add some polish and formatting\n",
        "plt.suptitle('Distribution of App Sizes Across Categories', fontsize=16, fontweight='bold', y=1.02, alpha=.7)\n",
        "plt.xlabel('Size', fontsize=12)\n",
        "plt.ylabel('Category', fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "sns.despine()\n",
        "\n",
        "ax = plt.gca()\n",
        "tick_locations = ax.get_yticks()\n",
        "\n",
        "formatted_labels = [label.replace('_', ' ').title() for label in size_order]\n",
        "\n",
        "plt.yticks(ticks=tick_locations, labels=formatted_labels)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xsoJEKr6ClZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For visual clarity, we'll focus on the top 15 categories by the number of apps.\n",
        "top_categories = df['Category'].value_counts().nlargest(15).index\n",
        "\n",
        "# Filter the DataFrame to only include these top categories.\n",
        "df_top_cat = df[df['Category'].isin(top_categories)]\n",
        "\n",
        "# Create a FacetGrid, which is ideal for creating a grid of similar plots.\n",
        "# We'll arrange the plots in a grid with 5 columns.\n",
        "g = sns.FacetGrid(df_top_cat, col=\"Category\", col_wrap=5, height=2.5, aspect=1.2)\n",
        "\n",
        "# Map a filled Kernel Density Estimate (KDE) plot onto each subplot of the grid.\n",
        "# This provides a smooth representation of the size distributions.\n",
        "g.map(sns.kdeplot, \"Size\", fill=True, alpha=0.5, color='royalblue')\n",
        "\n",
        "# Add a descriptive super-title to the entire figure.\n",
        "g.fig.suptitle('Density Distribution of Sizes for Top 15 Categories', y=1.03, fontsize=16, fontweight='bold')\n",
        "\n",
        "# Set individual subplot titles to be the category name.\n",
        "g.set_titles(\"{col_name}\", size=12)\n",
        "\n",
        "# Set the axis labels for clarity.\n",
        "g.set_axis_labels(\"Size\", \"Density\")\n",
        "\n",
        "# Ensure the layout is clean and titles/labels do not overlap.\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QSNLXknl8CXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visual Observation:** The boxplot suggests that there are indeed differences in the median sizes across categories. However, to confirm this with statistical rigor, we will perform a **Kruskal-Wallis H-test**. This non-parametric test is appropriate because we cannot assume the data is normally distributed, as suggested by KDE plots.\n",
        "\n",
        "- **Null Hypothesis (H₀):** The median sizes are the same across all app categories.\n",
        "\n",
        "- **Alternative Hypothesis (H₁):** At least one app category has a different median size.\n",
        "\n",
        "- **Significance Level (α):** 0.05"
      ],
      "metadata": {
        "id": "LWZARJpw7H9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the data for the Kruskal-Wallis test by creating a list of size series for each category.\n",
        "size_by_category = [df['Size'][df['Category'] == cat].dropna() for cat in categories]"
      ],
      "metadata": {
        "id": "oj6MeqXdmxWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform the Kruskal-Wallis H-Test and print the results\n",
        "h_statistic, p_value = kruskal(*size_by_category)\n",
        "\n",
        "print(\"--- Kruskal-Wallis H-Test Results ---\")\n",
        "print(f\"H-statistic: {h_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")"
      ],
      "metadata": {
        "id": "rnBch3XFmxUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since the p-value is significant, we proceed with Dunn's post-hoc test\n",
        "# to identify which specific category pairs are different.\n",
        "# The heatmap visualizes these pairwise comparisons.\n",
        "alpha = 0.05\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"\\nResult: The p-value is less than our significance level of 0.05.\")\n",
        "    print(\"Conclusion: We reject the null hypothesis. There is a statistically significant difference in median sizes across app categories.\")\n",
        "    print(\"\\nProceeding with Dunn's Post-Hoc Test to identify specific differences...\")\n",
        "\n",
        "    dunn_results = sp.posthoc_dunn(\n",
        "        df, val_col='Size', group_col='Category', p_adjust='bonferroni'\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        dunn_results, annot=False,\n",
        "        cmap='viridis_r',\n",
        "        cbar_kws={'label': 'Adjusted p-value'}\n",
        "    )\n",
        "    plt.suptitle(\n",
        "        \"Pairwise Comparison of App Sizes Across Categories (Dunn's Test)\",\n",
        "        fontsize=16,\n",
        "        y=1,\n",
        "        alpha=.7\n",
        "    )\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nResult: The p-value is greater than our significance level of 0.05.\")\n",
        "    print(\"Conclusion: We fail to reject the null hypothesis. There is no statistically significant evidence of a difference in median sizes across categories.\")"
      ],
      "metadata": {
        "id": "oGysoiTOmxQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:** The Kruskal-Wallis test yields a p-value of 0.0000, which is far below our significance level of 0.05. We therefore **reject the null hypothesis** and conclude that there is a statistically significant difference in median sizes across categories.\n",
        "\n",
        "This result validates our strategy of imputing the missing `Size` values with the **median size of each app's respective category.**"
      ],
      "metadata": {
        "id": "FmIfv5U4n7gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the median size for each category.\n",
        "category_size_medians = df.groupby('Category')['Size'].transform('median')\n",
        "\n",
        "# Fill the NaN values in the 'Size' column using the calculated category medians.\n",
        "df.fillna({'Size': category_size_medians}, inplace=True)"
      ],
      "metadata": {
        "id": "zEx6TkTDmxNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.4 Handling Final Minor Missing Values\n",
        "\n",
        "Finally, we address the few remaining missing values in the `Type`, `Current Ver`, and `Android Ver` columns."
      ],
      "metadata": {
        "id": "ASofiyksLVfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Isolate and inspect the rows with the remaining missing values to understand their context.\n",
        "df[df['Type'].isnull()]"
      ],
      "metadata": {
        "id": "JZFf4i8-niko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Current Ver'].isnull()]"
      ],
      "metadata": {
        "id": "rtf6VSPtniiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['Android Ver'].isnull()]"
      ],
      "metadata": {
        "id": "ytej-iT0MJky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision:** The number of affected rows (11 in total) is statistically insignificant (<0.2% of the dataset). Attempting to impute these categorical/text values would be unreliable. Therefore, the cleanest and most defensible approach is to remove these few rows entirely. This will not impact the overall data distribution or the conclusions of our analysis."
      ],
      "metadata": {
        "id": "dYoOZuloMPjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where 'Type', 'Current Ver', or 'Android Ver' are null.\n",
        "df.dropna(subset=['Type', 'Current Ver', 'Android Ver'], inplace=True)"
      ],
      "metadata": {
        "id": "H1I4iRxfISoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Saving the Cleaned Dataset\n",
        "\n",
        "The data cleaning and preprocessing phase is now complete. The resulting DataFrame is clean, free of missing values, and has the correct data types. We will save this cleaned dataset to a new CSV file for the exploratory data analysis (EDA) phase."
      ],
      "metadata": {
        "id": "_oQZUsTi99rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cleaned DataFrame to 'googleplaystore_cleaned.csv'.\n",
        "# `index=False` prevents pandas from writing the DataFrame index as a column.\n",
        "df.to_csv('googleplaystore_cleaned.csv', index=False)"
      ],
      "metadata": {
        "id": "YfauahSMUBBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Exploratory Data Analysis (EDA)\n",
        "\n",
        "With a clean and reliable dataset, we can now proceed with Exploratory Data Analysis. The goal of this phase is to understand the relationships between different features and to answer key questions about the app market. We will explore:\n",
        "\n",
        "- The overall distribution of numerical and categorical data.\n",
        "- Correlations between key metrics like `Rating`, `Reviews`, and `Installs`.\n",
        "- The performance of different app categories."
      ],
      "metadata": {
        "id": "0Ub8KNsScg2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.1 Data Overview and Feature Separation\n",
        "\n",
        "As a first step, we'll separate our features into numerical and categorical types for tailored analysis. We will also get a high-level view of the value distributions for our categorical features."
      ],
      "metadata": {
        "id": "eOMFVfYEGrDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate columns into numerical and categorical lists for easier analysis.\n",
        "# We use select_dtypes() for a robust way to identify column types.\n",
        "numeric_features = df.select_dtypes(include=['int64', 'Int64', 'float']).columns\n",
        "categorical_features = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "print(f\"Numeric features: {list(numeric_features)}\")\n",
        "print(f\"Categorical features: {list(categorical_features)}\")"
      ],
      "metadata": {
        "id": "00HL3eMAdPTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the percentage of unique values for each categorical feature.\n",
        "# This helps identify columns with high cardinality (like 'App') and those with\n",
        "# a few distinct groups (like 'Type').\n",
        "for feature in categorical_features:\n",
        "    print(df[feature].value_counts(normalize=True)*100)\n",
        "    print('-'*30)"
      ],
      "metadata": {
        "id": "2G4LF7xHfLnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.2 Correlation Analysis\n",
        "\n",
        "To understand the relationships between our key numerical metrics, we will visualize their correlations. A heatmap is an effective tool for this, as it provides a clear, color-coded overview of the strength and direction of these relationships."
      ],
      "metadata": {
        "id": "E7p8ucWJJp5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix for all numeric features.\n",
        "correlation_matrix = df[numeric_features].corr()\n",
        "\n",
        "# Create the heatmap with annotations to show the correlation values.\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)\n",
        "\n",
        "plt.title('Correlation Matrix of Numerical Features', fontsize=16, y=1.02)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rsc5TN3ex1N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight from Correlation Matrix:**\n",
        "\n",
        "There is a very strong positive correlation (0.64) between Reviews and Installs. This is intuitive: as more users install an app, the number of reviews it receives tends to increase.\n",
        "\n",
        "Other correlations are weak, suggesting that factors like Size or Price do not have a strong linear relationship with Rating or Installs on their own."
      ],
      "metadata": {
        "id": "gX-rr82kKRZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Univariate Analysis: Feature Distributions\n",
        "\n",
        "Next, we will examine the distribution of each feature individually to understand its characteristics.\n",
        "\n",
        "#### Numerical Feature Distributions\n",
        "We use Kernel Density Estimate (KDE) plots to visualize the shape of our numerical data."
      ],
      "metadata": {
        "id": "4NH9HnsFKi8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the density of numerical data using a loop to create subplots.\n",
        "# This provides a quick overview of the distribution (e.g., skewness, peaks) of each feature.\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.suptitle(\n",
        "    'Univariate Analysis of numerical features',\n",
        "    fontsize=20,\n",
        "    fontweight='bold',\n",
        "    alpha=0.6,\n",
        "    y=1\n",
        ")\n",
        "\n",
        "for i, feature in enumerate(numeric_features):\n",
        "    plt.subplot(5, 3, i+1)\n",
        "    sns.kdeplot(\n",
        "        x=df[feature],\n",
        "        fill=True,\n",
        "        color='royalblue'\n",
        "    )\n",
        "    sns.rugplot(\n",
        "        x=df[feature],\n",
        "        color='blue'\n",
        "    )\n",
        "\n",
        "    sns.despine()\n",
        "\n",
        "    plt.xlabel(\n",
        "        feature.replace('_', ' ').title(),\n",
        "        fontsize=10,\n",
        "        labelpad=5\n",
        "    )\n",
        "\n",
        "    plt.ylabel('Density', fontsize=9)\n",
        "    plt.yticks(fontsize=8)\n",
        "    plt.xticks(fontsize=8)\n",
        "\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "w4VCUMBSfmOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** Most numerical features, particularly Reviews, Installs, and Price, are heavily right-skewed. This means that the majority of apps have low values for these metrics, while a few highly successful apps have extremely high values (long tail). The Rating feature, however, is left-skewed, indicating that most apps have high ratings."
      ],
      "metadata": {
        "id": "-zYIifTJK3qS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorical Feature Distributions\n",
        "For key categorical features like `Type` and `Content Rating`, we use count plots to see the frequency of each category."
      ],
      "metadata": {
        "id": "aRJJyIH4LFmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the count of values for key categorical data.\n",
        "# We select a subset of features for clarity, as plotting all would be overwhelming.\n",
        "plt.figure(figsize=(15, 15))\n",
        "plt.suptitle(\n",
        "    'Univariate Analysis of categorical features',\n",
        "    fontsize=20,\n",
        "    fontweight='bold',\n",
        "    alpha=0.6,\n",
        "    y=1\n",
        ")\n",
        "\n",
        "categorical_features_for_visualization = ['Type', 'Content Rating']\n",
        "\n",
        "for i, feature in enumerate(categorical_features_for_visualization):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    sns.countplot(\n",
        "        x=df[feature],\n",
        "        color='royalblue'\n",
        "    )\n",
        "\n",
        "    sns.despine()\n",
        "\n",
        "    plt.xlabel(\n",
        "        feature.replace('_', ' ').title(),\n",
        "        fontsize=10,\n",
        "        labelpad=5\n",
        "    )\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "axyQx7_afmJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** The vast majority of apps on the Play Store are Free. The most common Content Rating is 'Everyone', followed by 'Teen'."
      ],
      "metadata": {
        "id": "HT9FNFe1LPU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.4 Answering Key Business Questions\n",
        "\n",
        "Now we move from general exploration to answering specific, targeted questions about the app market."
      ],
      "metadata": {
        "id": "BvaZpMfeLTN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see which App Categories are most represented in the dataset by count.\n",
        "# We'll visualize the top 5 for clarity.\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.suptitle(\n",
        "    'Top 5 Categories',\n",
        "    fontsize=20,\n",
        "    fontweight='bold',\n",
        "    alpha=0.6,\n",
        "    y=1\n",
        ")\n",
        "\n",
        "sns.barplot(\n",
        "    x=df['Category'].value_counts().head().index,\n",
        "    y=df['Category'].value_counts().head().values,\n",
        "    color='royalblue',\n",
        "    edgecolor='black',\n",
        "    linewidth=0.5,\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "sns.despine()\n",
        "\n",
        "plt.xlabel('Category', fontsize=10, labelpad=5)\n",
        "plt.ylabel('Count', fontsize=10, labelpad=5)\n",
        "\n",
        "plt.xticks(fontsize=8, rotation=45)\n",
        "plt.yticks(fontsize=8)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qjcW3bumfmHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** The `Family` and `Game` categories have the highest number of apps available on the store, indicating these are the most crowded and competitive markets."
      ],
      "metadata": {
        "id": "hbf_JJzNLd-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Which Category has the largest number of installations?\n",
        "2. Which are the top 5 most installed apps in each popular category?\n",
        "3. How many apps have 5 star rating?"
      ],
      "metadata": {
        "id": "s8v2KeROiBHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Which Category has the largest number of installations?\n",
        "While the 'Family' category has the most apps, it may not be the category with the most user engagement. To determine market size by reach, we will analyze the total number of installations per category."
      ],
      "metadata": {
        "id": "-nZA7aWGMS6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to format large numbers into a more readable format (e.g., 35B for 35 billion).\n",
        "def billions(x, pos):\n",
        "    return f'{x*1e-9:1.0f}B'\n",
        "\n",
        "# Group by category and sum the installs to find the most installed categories.\n",
        "installs_by_category = df.groupby('Category')['Installs'].sum().sort_values(ascending=False)\n",
        "\n",
        "top_n = 5\n",
        "\n",
        "# We create the dataframe for plotting\n",
        "installs_by_category_plot_df = installs_by_category.head(top_n).reset_index()\n",
        "installs_by_category_plot_df.columns = ['Category', 'Total Installs']\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.suptitle(\n",
        "    'Top 5 Categories by Installs',\n",
        "    fontsize=20,\n",
        "    fontweight='bold',\n",
        "    alpha=0.6,\n",
        "    y=1\n",
        ")\n",
        "\n",
        "# Create the bar plot\n",
        "ax = sns.barplot(\n",
        "    data=installs_by_category_plot_df,\n",
        "    x='Total Installs',\n",
        "    y='Category',\n",
        "    hue='Category',\n",
        "    orient='h',\n",
        "    palette='viridis'\n",
        ")\n",
        "\n",
        "# Apply the custom formatter to the x-axis\n",
        "ax.xaxis.set_major_formatter(plt.FuncFormatter(billions))\n",
        "\n",
        "plt.xlabel('Total Installations (in billions)', fontsize=12)\n",
        "plt.ylabel('Category', fontsize=12)\n",
        "plt.xticks(fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "sns.despine()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lo9BPUp1pypS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Which are the top 5 most installed apps in each popular category?\n",
        "To understand the key players in the largest markets, we will identify the top 5 most installed apps within the top 5 categories by installation count. This requires a more complex data manipulation workflow."
      ],
      "metadata": {
        "id": "RNKmjuDizmR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Most popular categories based on installations\n",
        "popular_categories = df.groupby('Category')['Installs'].sum().sort_values(ascending=False).head().index\n",
        "\n",
        "# Filter dataframe to include the top 5 categories\n",
        "popular_categories_df = df[df['Category'].isin(popular_categories)].copy().reset_index(drop=True)\n",
        "\n",
        "# Sort filtered df and groupby categories\n",
        "popular_categories_df = popular_categories_df.sort_values('Installs', ascending=False).groupby('Category').head().reset_index(drop=True)\n",
        "\n",
        "# Sort the dataframe with the top 5 most popular apps in each category by category and Installs\n",
        "final_df = popular_categories_df.sort_values(['Category', 'Installs'], ascending=[True, False]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "xV-ugT5kh2Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the results as subplots\n",
        "g = sns.catplot(\n",
        "    data=final_df,\n",
        "    x='Installs',\n",
        "    y='App',\n",
        "    col='Category',\n",
        "    col_wrap=3,\n",
        "    kind='bar',\n",
        "    color='royalblue',\n",
        "    height=4,\n",
        "    aspect=1.5,\n",
        "    sharex=False,\n",
        "    sharey=False,\n",
        ")\n",
        "\n",
        "g.fig.suptitle('Top 5 Most Installed Apps per Popular Category', y=1.03, fontsize=16, fontweight='bold', alpha=0.7)\n",
        "g.set_axis_labels('Total Installations', '')\n",
        "g.set_titles(\"Category: {col_name}\")\n",
        "g.fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2thilUiYh2Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the results as a single plot\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.barplot(\n",
        "    data=final_df,\n",
        "    x='Installs',\n",
        "    y='App',\n",
        "    hue='Category',\n",
        "    palette='tab10',\n",
        "    dodge=False\n",
        ")\n",
        "\n",
        "plt.title('Top 5 Most Installed Apps in Popular Categories', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Total Installations', fontsize=12)\n",
        "plt.ylabel('App', fontsize=12)\n",
        "plt.legend(title='Category', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eZ6eLQbY0kml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. How many apps have a perfect 5-star rating?\n",
        "Perfect ratings are rare and can be an indicator of either exceptional quality or potentially low review counts. Let's quantify how many apps achieve this."
      ],
      "metadata": {
        "id": "XqY7XE21yXHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the mask\n",
        "mask = df['Rating'] == 5\n",
        "\n",
        "# Apply the mask to our df\n",
        "five_star_apps_df = df[mask]\n",
        "\n",
        "# Check the length of the filtered df\n",
        "print(f\"Number of apps with 5 star rating: {len(five_star_apps_df)}\")"
      ],
      "metadata": {
        "id": "XCTC4BiKvh_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.5 Bonus Analysis: Perfect Ratings\n",
        "\n",
        "Let's dig deeper into the apps with perfect 5-star ratings.\n",
        "\n",
        "#### Which category has the most 5-star apps?\n",
        "We can visualize the distribution of these top-rated apps across categories to see if certain genres are more likely to receive perfect scores.\n",
        "\n"
      ],
      "metadata": {
        "id": "e6AqHuxvz5Co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by category & count the Apps with 5-star rating\n",
        "five_star_apps_by_category_df = five_star_apps_df.groupby('Category')['App'].count()\n",
        "\n",
        "# Grouping smaller slices into an 'Other' category is a great practice for pie chart readability.\n",
        "# Categories with 7 and less 5-star apps will pass to 'Other' category\n",
        "threshold = 8\n",
        "other_categories_mask = five_star_apps_by_category_df <= threshold\n",
        "\n",
        "# Create a new Series with the main categories\n",
        "main_categories = five_star_apps_by_category_df[~other_categories_mask]\n",
        "main_categories_list = main_categories.index.to_list()\n",
        "main_categories_list_formatted = [category.capitalize().replace('_', ' ') for category in main_categories_list]\n",
        "main_categories = pd.Series(main_categories.values, index=main_categories_list_formatted)\n",
        "\n",
        "# Sum the 'Other' categories and add it to the main Series\n",
        "other_categories_sum = five_star_apps_by_category_df[other_categories_mask].sum()\n",
        "main_categories['Other'] = other_categories_sum\n",
        "\n",
        "# Rename and Sort for a cleaner pie chart layout\n",
        "five_star_apps_by_category_df = main_categories.sort_values(ascending=False)\n",
        "\n",
        "\n",
        "# Visualize the results with a pie chart to show proportional distribution.\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.suptitle(\n",
        "    'Distribution of 5-star Apps by Category',\n",
        "    fontsize=15,\n",
        "    fontweight='bold',\n",
        "    alpha=0.7,\n",
        "    y=1\n",
        ")\n",
        "\n",
        "plt.pie(\n",
        "    five_star_apps_by_category_df,\n",
        "    labels=five_star_apps_by_category_df.index,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    colors=sns.color_palette('pastel'),\n",
        "    textprops={'fontsize': 8}\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uams4yBGzT4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight:** The `Family`, `Lifestyle` and `Medical` categories have the highest proportion of apps with a 5-star rating. This might suggest that apps in these categories serve a very specific, satisfied user base, or that they have fewer reviews, making a perfect score easier to maintain."
      ],
      "metadata": {
        "id": "jdU9h-CKQg2a"
      }
    }
  ]
}